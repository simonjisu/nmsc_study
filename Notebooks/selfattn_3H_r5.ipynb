{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "import nltk\n",
    "from konlpy.tag import Mecab\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0 if USE_CUDA else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Classification: Classify Good/Bad from movie reviews\n",
    "\n",
    "* Task: Many to One\n",
    "* Use: Attention(to know which words is important to classify the good/bad movie) + LSTM\n",
    "\n",
    "A STRUCTURED SELF-ATTENTIVE\n",
    "SENTENCE EMBEDDING: https://arxiv.org/pdf/1703.03130.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv('../data/ratings_train.txt', sep='\\t', usecols=[1, 2])\n",
    "# df_test = pd.read_csv('../data/ratings_test.txt', sep='\\t', usecols=[1, 2])\n",
    "# df_train.to_csv('../data/train_docs.txt', sep='\\t', index=False, header=False)\n",
    "# df_test.to_csv('../data/test_docs.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = Mecab()\n",
    "tagger = tagger.morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REVIEW = Field(tokenize=tagger, use_vocab=True, lower=True, #init_token=\"<s>\", eos_token=\"</s>\", \n",
    "               include_lengths=True, batch_first=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False, preprocessing=lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = TabularDataset.splits(\n",
    "                   path=\"../data/\", train='train_docs.txt', validation=\"test_docs.txt\",\n",
    "                   format='tsv', fields=[('review', REVIEW), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 50000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53078"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Vocaburary\n",
    "REVIEW.build_vocab(train_data)\n",
    "len(REVIEW.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make iterator for splits\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train_data, test_data), batch_size=batch_size, device=DEVICE, # device -1 : cpu, device 0 : 남는 gpu\n",
    "    sort_key=lambda x: len(x.review), sort_within_batch=True, repeat=False) # x.TEXT 길이 기준으로 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bidirec_LSTM(nn.Module):\n",
    "    def __init__(self, V, D, H, H_f, O, da, r, num_layers=3, bidirec=False, use_cuda=False):\n",
    "        \"\"\"\n",
    "        V: input_size = vocab_size\n",
    "        D: embedding_size\n",
    "        H: hidden_size\n",
    "        H_f: hidden_size (fully-connected)\n",
    "        O: output_size (fully-connected)\n",
    "        da: attenion_dimension (hyperparameter)\n",
    "        r: keywords (different parts to be extracted from the sentence)\n",
    "        \"\"\"\n",
    "        super(bidirec_LSTM, self).__init__()\n",
    "        self.r = r\n",
    "        self.da = da\n",
    "        self.hidden_size = H\n",
    "        self.num_layers = num_layers\n",
    "        self.USE_CUDA = use_cuda\n",
    "        if bidirec:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "        \n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        self.lstm = nn.LSTM(D, H, num_layers, batch_first=True, bidirectional=bidirec)\n",
    "        self.attn = nn.Linear(self.num_directions*H, self.da, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.attn2 = nn.Linear(self.da, self.r, bias=False)\n",
    "        self.attn_dist = nn.Softmax(dim=2)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(r*H*self.num_directions, H_f),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H_f, O),\n",
    "        )\n",
    "            \n",
    "    def init_LSTM(self, batch_size):\n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        hidden = Variable(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size))\n",
    "        cell = Variable(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size))\n",
    "        if self.USE_CUDA:\n",
    "            hidden = hidden.cuda()\n",
    "            cell = cell.cuda()\n",
    "        return hidden, cell\n",
    "    \n",
    "    def penalization_term(self, A):\n",
    "        \"\"\"\n",
    "        A : B, r, T\n",
    "        Frobenius Norm \n",
    "        \"\"\"\n",
    "        eye = Variable(torch.eye(A.size(1)).expand(A.size(0), self.r, self.r)) # B, r, r\n",
    "        if self.USE_CUDA:\n",
    "            eye = eye.cuda()\n",
    "        P = torch.bmm(A, A.transpose(1, 2)) - eye # B, r, r\n",
    "        loss_P = ((P**2).sum(1).sum(1) + 1e-10) ** 0.5\n",
    "        loss_P = torch.sum(loss_P) / A.size(0)\n",
    "        return loss_P\n",
    "        \n",
    "    def forward(self, inputs, inputs_lengths):\n",
    "        \"\"\"\n",
    "        inputs: B, T, V\n",
    "         - B: batch_size\n",
    "         - T: max_len = seq_len\n",
    "         - V: vocab_size\n",
    "        inputs_lengths: length of each sentences\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs)  # B, T, V  --> B, T, D\n",
    "        hidden, cell = self.init_LSTM(inputs.size(0))  # num_layers * num_directions, B, H\n",
    "        \n",
    "        # 패딩된 문장을 패킹(패딩은 연산 안들어가도록)\n",
    "        packed = pack_padded_sequence(embed, inputs_lengths.tolist(), batch_first=True)\n",
    "        # packed: B * T, D\n",
    "        output, (hidden, cell) = self.lstm(packed, (hidden, cell))\n",
    "        # output: B * T, 2H\n",
    "        # hidden, cell: num_layers * num_directions, B, H\n",
    "        \n",
    "        # 패킹된 문장을 다시 unpack\n",
    "        output, output_lengths = pad_packed_sequence(output, batch_first=True) \n",
    "        # output: B, T, 2H\n",
    "\n",
    "        # Self Attention\n",
    "        a1 = self.attn(output)  # Ws1(B, da, 2H) * output(B, T, 2H) -> B, T, da\n",
    "        tanh_a1 = self.tanh(a1)  # B, T, da\n",
    "        score = self.attn2(tanh_a1)  # Ws2(B, r, da) * tanh_a1(B, T, da) -> B, T, r\n",
    "        self.A = self.attn_dist(score.transpose(1, 2))  # B, r, T\n",
    "        self.M = self.A.bmm(output)  # B, r, T * B, T, 2H -> B, r, 2H \n",
    "        \n",
    "        # Penalization Term\n",
    "        loss_P = self.penalization_term(self.A)\n",
    "        \n",
    "        output = self.fc(self.M.view(self.M.size(0), -1)) # B, r, 2H -> resize to B, r*2H -> B, H_f -> Relu -> B, 1\n",
    "        \n",
    "        return output, loss_P\n",
    "    \n",
    "    def predict(self, inputs, inputs_lengths):\n",
    "        preds, _ = self.forward(inputs, inputs_lengths)\n",
    "        return F.sigmoid(preds).ge(0.5).long().view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V = len(REVIEW.vocab)\n",
    "D = 100\n",
    "H = 200\n",
    "H_f = 1000\n",
    "O = 1\n",
    "da = 300\n",
    "r = 5\n",
    "num_layers = 3\n",
    "num_directions = 2\n",
    "bidirec = True\n",
    "batch_size = 64\n",
    "# weight_decay_rate = 0.0001\n",
    "LR = 0.01\n",
    "STEP = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = bidirec_LSTM(V, D, H, H_f, O, da, r, num_layers=num_layers, bidirec=bidirec, use_cuda=USE_CUDA)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=weight_decay_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(gamma=0.1, milestones=[2, 7, 12], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simonjisu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/15] loss: 0.9496\n",
      "[2/15] loss: 0.8626\n",
      "[3/15] loss: 0.8415\n",
      "[4/15] loss: 0.8402\n",
      "[5/15] loss: 0.8390\n",
      "[6/15] loss: 0.7450\n",
      "[7/15] loss: 0.4578\n",
      "[8/15] loss: 0.3275\n",
      "[9/15] loss: 0.2908\n",
      "[10/15] loss: 0.2629\n",
      "[11/15] loss: 0.2356\n",
      "[12/15] loss: 0.2132\n",
      "[13/15] loss: 0.1974\n",
      "[14/15] loss: 0.1939\n",
      "[15/15] loss: 0.1914\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for step in range(STEP):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        inputs, lengths = batch.review\n",
    "        targets = batch.label\n",
    "        # prevent length = 0\n",
    "        if 0 in lengths:\n",
    "            idxes = torch.arange(inputs.size(0))\n",
    "            if USE_CUDA:\n",
    "                idxes = idxes.cuda()\n",
    "            mask = Variable(idxes[lengths.ne(0)].long())\n",
    "\n",
    "            inputs = inputs.index_select(0, mask)\n",
    "            lengths = lengths.masked_select(lengths.ne(0))\n",
    "            targets = targets.index_select(0, mask)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds, loss_P = model(inputs, lengths)\n",
    "        \n",
    "        loss = loss_function(preds.view(-1), targets.float()) + loss_P\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    string = '[{}/{}] loss: {:.4f}'.format(step+1, STEP, np.mean(losses))\n",
    "    print(string)\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../model/self_attn_3H_r5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = bidirec_LSTM(V, D, H, H_f, O, da, r, num_layers=num_layers, bidirec=bidirec, use_cuda=USE_CUDA)\n",
    "# if USE_CUDA:\n",
    "#     model = model.cuda()\n",
    "# model.load_state_dict(torch.load('../model/self_attn_3H_r5.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.85096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simonjisu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "num_equal=0\n",
    "for i, batch in enumerate(test_iter):\n",
    "    inputs, lengths = batch.review\n",
    "    targets = batch.label\n",
    "    if 0 in lengths:\n",
    "        idxes = torch.arange(inputs.size(0))\n",
    "        if USE_CUDA:\n",
    "            idxes = idxes.cuda()\n",
    "        mask = Variable(idxes[lengths.ne(0)].long())\n",
    "\n",
    "        inputs = inputs.index_select(0, mask)\n",
    "        lengths = lengths.masked_select(lengths.ne(0))\n",
    "        targets = targets.index_select(0, mask)\n",
    "    preds = model.predict(inputs, lengths)\n",
    "    num_equal += torch.eq(preds, targets).sum().data[0]\n",
    "\n",
    "print(\"Accuracy : \" , num_equal / len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference) Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a sentence, which has n tokens, represented in a sequence of word embeddings.\n",
    "$$S = (w_1, w_2, \\cdots, w_n)\\qquad\\qquad (1)$$\n",
    "Here $w_i$, is a vector standing for a $d$ dimentional word embedding for the $i$-th word in the sentence.\n",
    "$S$ is thus a sequence represented as a 2-D matrix, which concatenates all the word embeddings\n",
    "together. $S$ should have the shape $n$-by-$d$.\n",
    "\n",
    "Now each entry in the sequence $S$ are independent with each other. To gain some dependency between\n",
    "adjacent words within a single sentence, we use a bidirectional LSTM to process the sentence:\n",
    "$$\\begin{aligned}\n",
    "\\overrightarrow{h_t} &= \\overrightarrow{LSTM}(w_t, \\overrightarrow{h_{t-1}})\\qquad\\qquad (2) \\\\\n",
    "\\overleftarrow{h_t} &= \\overleftarrow{LSTM}(w_t, \\overleftarrow{h_{t-1}})\\qquad\\qquad (3)\n",
    "\\end{aligned}$$\n",
    "\n",
    "And we concatenate each $\\overrightarrow{h_t}$ with $\\overleftarrow{h_t}$ to obtain a hidden state $h_t$. Let the hidden unit number for each unidirectional LSTM be $u$. For simplicity, we note all the $n$ $h_t$s as $H$, who have the size $n$-by-2$u$.\n",
    "$$H = (h_1, h_2, \\cdots, h_n)$$\n",
    "\n",
    "Our aim is to encode a variable length sentence into a fixed size embedding. We achieve that by\n",
    "choosing a linear combination of the n LSTM hidden vectors in $H$. Computing the linear combination\n",
    "requires the self-attention mechanism. The attention mechanism takes the whole LSTM hidden\n",
    "states $H$ as input, and outputs a vector of weights $a$:\n",
    "$$a = softmax(w_{s2} \\tanh (W_{s1}H^T))$$\n",
    "\n",
    "Here $W_{s1}$ is a weight matrix with a shape of $d_a$-by-2$u$. and $w_{s2}$ is a vector of parameters with\n",
    "size $d_a$, where $d_a$ is a hyperparameter we can set arbitrarily. Since $H$ is sized $n$-by-2$u$, the annotation vector a will have a size $n$. the $softmax()$ ensures all the computed weights sum up to 1.\n",
    "Then we sum up the LSTM hidden states $H$ according to the weight provided by a to get a vector representation m of the input sentence.\n",
    "\n",
    "This vector representation usually focuses on a specific component of the sentence, like a special set of related words or phrases. So it is expected to reflect an aspect, or component of the semantics in a sentence. However, there can be multiple components in a sentence that together forms the overall semantics of the whole sentence, especially for long sentences. (For example, two clauses linked together by an ”and.”) Thus, to represent the overall semantics of the sentence, we need multiple $m$’s that focus on different parts of the sentence. Thus we need to perform multiple hops of attention.\n",
    "\n",
    "Say we want $r$ different parts to be extracted from the sentence, with regard to this, we extend the\n",
    "$w_{s2}$ into a $r$-by-$d_a$ matrix, note it as $W_{s2}$, and the resulting annotation vector a becomes annotation matrix $A$. Formallly,\n",
    "$$A=softmax(W_{s2}tanh(W_{s1}H^T))$$\n",
    "\n",
    "Here the $softmax()$ is performed along the second dimension of its input. We can deem Equation 6 as a 2-layer MLP without bias, whose hidden unit numbers is $d_a$, and parameters are $\\{W_{s2}, W_{s1}\\}$.\n",
    "The embedding vector $m$ then becomes an $r$-by-2$u$ embedding matrix $M$. We compute the $r$\n",
    "weighted sums by multiplying the annotation matrix $A$ and LSTM hidden states $H$, the resulting\n",
    "matrix is the sentence embedding:\n",
    "$$M=AH$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
